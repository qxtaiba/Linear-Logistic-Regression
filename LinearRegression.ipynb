{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\tImporting the standard libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\tImporting the dataset\n",
    "#       a.\tUsing sklearn\n",
    "\n",
    "bostonData = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13\n",
      "Number of datapoints: 506\n",
      "Name of features: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "First Two Rows: [[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 3.\tViewing the dataset\n",
    "#     a.\t“the number of features”\n",
    "#     b.\t“the features names:”\n",
    "#     c.\t“the number of examples”\n",
    "\n",
    "#  create X and Y variables - X holding the .data and Y holding .target \n",
    "x = bostonData.data\n",
    "y = bostonData.target\n",
    "\n",
    "numFeatures = x.shape[1]\n",
    "numPoints = x.shape[0]\n",
    "featureNames = bostonData.feature_names\n",
    "firstTwoRows = x[0:2]\n",
    "\n",
    "print(\"Number of features:\",numFeatures)\n",
    "print(\"Number of datapoints:\",numPoints)\n",
    "print(\"Name of features:\",featureNames)\n",
    "print(\"First Two Rows:\",firstTwoRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\tTransfer to polynomial feature\n",
    "#     a.\tUsing API\n",
    "#     b.\tPrint the shape of new freature \n",
    "\n",
    "def transformPolyFeature(x):\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "    return  poly.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\tImplementing the function that return the close form w for linear regression \n",
    "#     a.\tReturn w values\n",
    "\n",
    "# 6.\tImplementing the function that return the close form w for ridge regression \n",
    "#     a.\tReturn w values\n",
    "\n",
    "def linearRidgeRegressionClosedForm(xTrainTemp,yTrain, lambdaVar):\n",
    "    if len(yTrain.shape) == 1: \n",
    "        yTrain = yTrain[:, np.newaxis]\n",
    "        \n",
    "    A = lambdaVar * np.identity(xTrainTemp.shape[1]) \n",
    "    B = np.linalg.pinv(np.matmul(np.transpose(xTrainTemp),xTrainTemp)+A)\n",
    "    \n",
    "    return np.matmul(np.matmul(B,np.transpose(xTrainTemp)),yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\tImplementing the evaluation function\n",
    "#     a.\tReturn the trainError and testError values\n",
    "\n",
    "def errorEvaluation(xTrainTemp, xTestTemp, yTrain, yTest, wCoeffecients):\n",
    "   \n",
    "    predTrain = (np.matmul(xTrainTemp,wCoeffecients).squeeze()) * np.std(yTrain) + np.mean(yTrain)\n",
    "\n",
    "    predTest = (np.matmul(xTestTemp,wCoeffecients).squeeze()) * np.std(yTrain) + np.mean(yTrain)\n",
    "    \n",
    "    tempTrain = (yTrain-predTrain)**2\n",
    "    tempTest = (yTest-predTest)**2\n",
    "    \n",
    "    sizeTrain = yTrain.shape[0]\n",
    "    sizeTest = yTest.shape[0]\n",
    "    \n",
    "    trainError = np.sum(tempTrain) / sizeTrain\n",
    "    testError = np.sum(tempTest) / sizeTest\n",
    "    return trainError, testError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.\tFinish writting the k_fold_cross_validation function. \n",
    "#     a.\tReturns the average training error and average test error from the k-fold cross validation\n",
    "#     b.\tuse Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "#     c.\tcentering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "#     d.\tscaling the data matrix\n",
    "#     e.\tdetermine the training error and the test error\n",
    "\n",
    "def k_fold_cross_validation(k,x,y,lambdaVar):\n",
    "    \n",
    "    kFold = KFold(n_splits = k, shuffle = True)\n",
    "    trainingErrorValues = []\n",
    "    testingErrorValues = []\n",
    "    \n",
    "    for trainingDataIndex, testingDataIndex in kFold.split(x):\n",
    "  \n",
    "        xTrain = x[trainingDataIndex]\n",
    "        xTest = x[testingDataIndex]\n",
    "        \n",
    "        yTrain, yTest = y[trainingDataIndex], y[testingDataIndex]\n",
    "\n",
    "        scaleVal = preprocessing.StandardScaler().fit(xTrain)\n",
    "        xTrain = scaleVal.transform(xTrain)\n",
    "        xTest = scaleVal.transform(xTest)\n",
    "\n",
    "        xTrainMean = np.mean(xTrain)\n",
    "        xTrainStd = np.std(xTrain)\n",
    "        xTrainStd = np.where(xTrainStd == 0, 1, xTrainStd)\n",
    "        \n",
    "        xTrain = (xTrain - xTrainMean)/xTrainStd\n",
    "        xTrainTemp = np.ones((xTrain.shape[0], x.shape[1] + 1))\n",
    "        xTrainTemp[:, :x.shape[1]] = xTrain\n",
    "        \n",
    "        xTest = (xTest - xTrainMean)/xTrainStd\n",
    "        xTestTemp = np.ones((xTest.shape[0], x.shape[1] + 1))\n",
    "        xTestTemp[:,:x.shape[1]] = xTest\n",
    "        \n",
    "        \n",
    "        yTrainMean = np.mean(yTrain)\n",
    "        yTrainStd = np.std(yTrain)\n",
    "        yTrainTemp = (yTrain - yTrainMean)/yTrainStd\n",
    "        \n",
    "        wCoeffecients = linearRidgeRegressionClosedForm(xTrainTemp,yTrainTemp,lambdaVar)\n",
    "        \n",
    "        \n",
    "        trainingErrorValueTemp, testingErrorValueTemp = errorEvaluation(xTrainTemp, xTestTemp, yTrain, yTest, wCoeffecients)\n",
    "   \n",
    "\n",
    "\n",
    "        trainingErrorValues.append(trainingErrorValueTemp)\n",
    "        testingErrorValues.append(testingErrorValueTemp)\n",
    "\n",
    "    return trainingErrorValues, testingErrorValues, wCoeffecients\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Training Error 21.794178077854117\n",
      "Linear Testing Error 23.877623713602993\n",
      "\n",
      "Best Lambda: 10.0\n",
      "\n",
      "Ridge Training Error 21.898427214241682\n",
      "Ridge Testing Error 23.503067330575597\n"
     ]
    }
   ],
   "source": [
    "# print the error for the both linear regression and ridge regression\n",
    "# the error should include both training error and testing error\n",
    "\n",
    "trainingErrorValuesLinear, testingErrorValuesLinear, wCoeffecients = k_fold_cross_validation(10,x,y,0)\n",
    "\n",
    "print(\"Linear Training Error\",sum(trainingErrorValuesLinear)/len(trainingErrorValuesLinear))\n",
    "print(\"Linear Testing Error\",sum(testingErrorValuesLinear)/len(testingErrorValuesLinear))\n",
    "\n",
    "lambdaValRange = np.logspace(1, 7, num=13)\n",
    "finLambdaVar = 0\n",
    "minTestError = float(\"inf\")\n",
    "minTrainError = float(\"inf\")\n",
    "for i in lambdaValRange:\n",
    "    trainingErrorValuesRidge, testingErrorValuesRidge, wCoeffecients = k_fold_cross_validation(10,x,y,i)\n",
    "    averageTestingError = np.sum(testingErrorValuesRidge)/10\n",
    "    averageTrainingError = np.sum(trainingErrorValuesRidge)/10\n",
    "    if minTestError > averageTestingError:\n",
    "        minTestError = averageTestingError\n",
    "        minTrainError = averageTrainingError\n",
    "        finLambdaVar = i\n",
    "print(\"\\nBest Lambda:\", finLambdaVar)\n",
    "print(\"\\nRidge Training Error\",minTrainError)\n",
    "print(\"Ridge Testing Error\",minTestError)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Training Error 5.758004343418935\n",
      "Linear Testing Error 13.703209742296433\n",
      "\n",
      "Best Lambda: 10.0\n",
      "\n",
      "Ridge Training Error 10.04027864980662\n",
      "Ridge Testing Error 13.770060964084388\n"
     ]
    }
   ],
   "source": [
    "xTransformed = transformPolyFeature(x)\n",
    "xTransformed = xTransformed[:,1:]\n",
    "\n",
    "trainingErrorValuesLinear, testingErrorValuesLinear, wCoeffecients = k_fold_cross_validation(10,xTransformed,y,0)\n",
    "\n",
    "print(\"Linear Training Error\",sum(trainingErrorValuesLinear)/len(trainingErrorValuesLinear))\n",
    "print(\"Linear Testing Error\",sum(testingErrorValuesLinear)/len(testingErrorValuesLinear))\n",
    "\n",
    "lambdaValRange = np.logspace(1, 7, num=13)\n",
    "finLambdaVar = 0\n",
    "minTestError = float(\"inf\")\n",
    "minTrainError = float(\"inf\")\n",
    "finWCoeffecients = 0\n",
    "for i in lambdaValRange:\n",
    "    trainingErrorValuesRidge, testingErrorValuesRidge, wCoeffecients = k_fold_cross_validation(10,xTransformed,y,i)\n",
    "    averageTestingError = np.sum(testingErrorValuesRidge)/10\n",
    "    averageTrainingError = np.sum(trainingErrorValuesRidge)/10\n",
    "    if minTestError > averageTestingError:\n",
    "        minTestError = averageTestingError\n",
    "        minTrainError = averageTrainingError\n",
    "        finLambdaVar = i\n",
    "        finWCoeffecients = wCoeffecients\n",
    "print(\"\\nBest Lambda:\", finLambdaVar)\n",
    "print(\"\\nRidge Training Error\",minTrainError)\n",
    "print(\"Ridge Testing Error\",minTestError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters - K: 10\n",
      "\n",
      "Model Parameters - Lambda: 10.0\n",
      "\n",
      "Model Parameters - W:\n",
      " [ 2.08304526e-02 -5.26832327e-02  6.94457848e-02  6.14940224e-02\n",
      "  3.21368118e-02  2.02638096e-01  7.15256890e-02 -1.37354001e-01\n",
      "  1.49821336e-01  3.96991886e-02 -6.93921182e-03  2.35728817e-02\n",
      "  1.02910451e-02  8.12909395e-02  4.19910867e-02  2.46448160e-02\n",
      "  2.41958481e-01 -6.13435305e-02 -1.11864257e-01  1.69899071e-02\n",
      " -7.50439693e-02 -1.21787032e-02  4.78814376e-03  1.16759402e-02\n",
      " -6.00477515e-02  4.57134224e-03  6.45987168e-02 -6.93777691e-02\n",
      " -1.01164018e-02 -2.68089877e-02  7.93211237e-02  3.54475548e-03\n",
      " -2.35992880e-03 -2.06110893e-03  6.00961939e-02  5.42073390e-02\n",
      " -4.29594544e-02 -7.15508440e-02  6.13567385e-02 -2.06079330e-02\n",
      "  4.66078611e-02 -1.02294060e-01  6.70302680e-02 -7.73383887e-02\n",
      "  8.75537291e-02  1.01014030e-01 -3.54060578e-02  7.50967595e-02\n",
      " -1.42605935e-01  6.14940224e-02 -1.98640973e-01 -1.27416381e-01\n",
      "  9.42868915e-02  4.57460857e-02 -1.11258642e-01 -6.94199628e-02\n",
      "  7.08335941e-02  1.42760412e-01 -7.32848245e-02 -1.66433894e-02\n",
      " -9.62497742e-02 -3.86242260e-02 -1.13740489e-01 -1.52656619e-02\n",
      " -4.81577959e-02 -9.21123761e-02  3.83865452e-02 -3.44882726e-02\n",
      "  4.47933993e-01 -6.85800805e-02  3.15370279e-02 -1.57049772e-01\n",
      " -2.50053179e-01 -1.65102766e-01  1.96081827e-01 -3.73316653e-01\n",
      "  2.60130955e-02 -1.59897192e-02  7.55786654e-02  9.02943413e-03\n",
      " -2.88723418e-02 -5.32545964e-02 -1.08670397e-01  1.00397617e-01\n",
      "  5.56995286e-02 -9.49752691e-02 -5.16351722e-03 -8.30755075e-02\n",
      "  1.48998585e-01  7.81077265e-03  5.78702482e-02  1.26570065e-01\n",
      "  5.56735174e-02 -1.34152180e-01  1.25237406e-02  6.55733381e-02\n",
      " -1.08108185e-02 -1.32675353e-01  1.35048716e-02 -1.59240804e-02\n",
      "  1.07040808e-02 -7.77538143e-02 -1.23849893e-01  3.58478174e-01\n",
      "  7.32747196e-15]\n"
     ]
    }
   ],
   "source": [
    "# I would select a closed form ridge regression model with a second degree\n",
    "# polynomial transormation. This is because it produces the lowest average\n",
    "# testing error. The paramters for this model are as follows: \n",
    "\n",
    "print(\"Model Parameters - K:\",10)\n",
    "print(\"\\nModel Parameters - Lambda:\",finLambdaVar)\n",
    "print(\"\\nModel Parameters - W:\\n\",finWCoeffecients.squeeze())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
